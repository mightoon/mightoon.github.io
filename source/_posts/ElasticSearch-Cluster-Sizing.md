---
title: "[ElasticSearch] ES 集群规模评估"
date: 2020-4-24 11:22:43
description: ElasticSearch 在我们的数据中心中已经小范围使用了较长一段时间，最近准备规划一个专有的 infrastructure, 将 ES cluster 移入，并用于收集和处理整个数据中心中的所有设备和应用程序的 log. 本篇文章记录了新的 Cluster 的规模的估计以及规划的过程。
categories: ElasticSearch
tags: Sizing
---

作为日志的收集、处理和存储，ELK 在我们的数据中心中已经运行了一年有余，虽然只用于局部的项目，但确实起到了很好的 log 集中存储，快速查询，直观展示的作用。随着对 ELK 技术栈的进一步熟悉，我们打算扩展 ELK 的监控范围，覆盖整个数据中心的所有设备以及之上运行的各种 enterprise application，同时也打算对收集到的数据做一些 business insights 方面的尝试，为此需要为 ELK 规划单独的资源。因为我们的数据中心并不算大，所以主要涉及的是 ElasticSearch Cluster的扩展。我们的做法大致分为：
- 首先通过现有的数据规模和 ES 运行状况，估算出适用于整个数据中心的 ES 集群规模
- 然后通过 ES benchmark 测试工具验证新集群的性能
- 接着将现有日志迁移到新集群并扩展 beat 端，更新 logstash pipeline，使日志收集覆盖所有的设备和应用 

## 需要提前明确的问题
在评估 ES Cluster 之前，有几个基础性问题需要明确，诸如：
- 主要的应用场景是什么？
- 数据的规模有多大？
- 数据的生命周期是怎样的？
- 准备使用怎样的ES集群架构？
- 以何种方式存储数据？
这些问题的答案一部分来自于 ES cluster 的最佳实践，一部分参考我们正在运行的小规模集群（下文叫做 PoC cluster）

## PoC 集群的数据规模
目前的 PoC 集群是一个小型的三节点 ES Cluster，由三个虚机构成，收集并存储一个在研项目的 log
- 虚机规格
  - 4core CPU
  - 8GB MEM
  - 500GB SSD
- 监控的设备
  - 40台物理机及虚机
  - 7台交换机
  - 8台存储设备
  - 一个application
- 数据量
  - 2.2GB/天
  - 数据保存6个月
PoC 非常简易，没有考虑数据分层，也没作数据的持久化保存，但是通过对 PoC infra 的长期监控，我们得到改规模下日志的数据量，虽然不能精准地类推，但起码是一个不错的参考。

## 需求
- 使用场景
  - 主要依然来自于 log 的收集和存储，尽量实时写入并能实时查询
  - 涉及一些分析型的场景，因此对历史数据有读的需求，但是不会很频繁
- 规模：我们整个数据中心当前的规模，经过测算，大概 15 倍于 PoC 环境的规模，即：
  - 600台物理机和虚机
  - 90台数据中心交换机
  - 50台存储设备
  - 10个左右的中等规模的应用系统
- 数据存储：数据需要以经济的方式长期保存

## 集群架构的选择
ES 典型的集群架构分为`混合部署架构`和`分层部署架构`，在于是否将各个 node 的角色明确分开。由于我们的监控规模不大，即便是覆盖整个数据中心，也只需要中小规模的集群，所以在架构上，没有把 master node，cooridinating node 等角色独立出来。这样的好处是各个节点的结构一致，部署相对简单。当然，在后期如果发现 node 上处理 data 的压力过大，影响到其他角色，我们也会考虑到时候把必要的角色分离出来。 

## 集群规模评估
类似 PoC 的运行强度，根据以上数据中心的规模，大概每天产生 `30GB` 的数据量。使用`一个副本`来保证数据的冗余。

### 内存的考虑
在 PoC 的运行过程中，我们曾遇到两次 `circuit break` 的问题。实际上 ES 会将大量数据缓存在堆内存中，而默认的堆内存配置仅仅为 1GB，这会造成很容易因为内存不足而触发 circuit break. 根据 ES 的推荐，我们把堆内存配置为4GB，好了很多，后来一年仅观察到一次 break. 考虑到新的集群需要处理多得多的数据，同时 ES 建议

> 堆内存配置为节点内存的一半且不超过 32G

所以我们把单个节点的内存确定为 64G. 

### 关于数据的生命周期
引入`数据生命周期`的管理，主要是为了在数据长期保存的前提下节约节点资源，以及利用我们现有的存储设备。Cluster 划分为 hot 节点和 warm 节点。Hot 节点承担较多的写入，直接配置 SSD，数据保存30天后，转入 warm 节点至少保存1年。 转入的时候可以通过 merge data，shrink 等方式节省存储空间。同时，warm 节点使用我们数据中心现成的中端存储设备，节省 SSD，数据也更安全。

### 节点的规格与数量
综合上面的考虑，我们将**节点规格**确定为：
- Hot 节点: 8core CPU, 64G MEM, 1.6T SSD
- Warm 节点： 8core CPU, 32G MEM, 外部存储

根据 ES 的建议算法，得出两类**节点数量**：
- Hot 节点
  - 数据总量 (GB) =（30GB x 30 天 * 2）= 1800GB
  - 存储总量 (GB) = 1800GB x ( 1 + 0.15磁盘水位阈值 + 0.1误差幅度 ) = 2250GB
  - 数据节点总数 = ROUNDUP(2250 / 64 / 30) + 1 = **3 个节点**
- Warm 节点
  - 数据总量 (GB) =（30GB x 365 天 * 2）= 21900GB
  - 存储总量 (GB) = 21900GB x (1+0.15+0.1) = 27375GB
  - 数据节点总数 = ROUNDUP(27375 / 32 / 160) + 1 = **6 个节点**


